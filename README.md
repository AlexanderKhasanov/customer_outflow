# Отток клиентов

## Постановка задачи

Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых. Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Вам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком. 
Постройте модель с предельно большим значением *F1*-меры. Чтобы сдать проект успешно, нужно довести метрику до 0.59. Проверьте *F1*-меру на тестовой выборке самостоятельно.
[Источник данных](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)

## Используемые технологие

Проект написан на языке Python с использованием следующих библиотек: pandas, scikit-learn, numpy, matplotlib.

## Результаты

В ходе работы были исследованы исторические данные о поведении клиентов и расторжении договоров с банком. На первоб этапе работы была проведена предобработка данных (устранены все пропуски, закодированы категориальный признаки и нормированы количественные). Стоит отметить, что в данных содержится дисбаланс классов (в датасете преобладают данные с классом `0`). Датасет был разделен на 3 выборки:
* Тренировочная - 60%  
* Валидационная - 20%  
* Тестовая - 20%  

На втором этапе работы на данных с дисбалансом классов было обучено 3 модели:
* Модель решающего дерева. Показала лучший результат при максимальной глубине равной 2. Значение метрики F1 на валидационной выборке составило `0.5007` .  
* Модель случайного дерева. Показала лучший результат при максимальной глубине равной 22 и количестве решателей равным 45. Значение метрики F1 на валидационной выборке составило `0.6012` .  
* Модель логистической регрессии, обученная на 100 итерациях, на валидационной выборке показала значение метрики F1 равное `0.3201` .  

На третьем этапе указанный выше модели обучались на тренировочной выборке, на которой был устранен дисбаланс классов. Уравновешивание выборки выполнялось двумя способами.
1. Взвешивание классов. При данном методе модели показали следующие результаты:
    * Модель решающего дерева показала лучший результат при максимальной глубине равной 6. Значение метрики F1 на валидационной выборке выросло до `0.5875` . Площадь под ROC кривой для данной модели составила `0.8348`, что говорит о достаточно хорошей разделяющей способности данной модели.  
    * Модель случайного леса показала лучший результат при максимальной глубине равной 10 и количестве решателей равным 135. Значение метрики F1 на валидационной выборке выросло до `0.6434` . Площадь под ROC кривой для данной модели составила `0.8579`, что говорит о хорошей разделяющей способности данной модели. Это лучший результат из всех трех моделей.  
    * Модель логистической регрессии, обученная на 100 итерациях, на валидационной выборке показала значение метрики F1 выросло до `0.481` . Площадь под ROC кривой для данной модели составила `0.7602`. Разделяющая способность данной модели худшая из всех трех.  
    
Таким образом, лучший результат показала модель случайного леса и имеет лучшую разделяющую способнось.

2. С помощью увеличения выборки. Положительный класс был увеличен в 4 раза. В результате удалось добиться того, что положительный класс составлял `50.59%` обучающей выборки. При данном методе модели показали следующие результаты: 
    * Модель решающего дерева показала лучший результат при максимальной глубине равной 6. Значение метрики F1 на валидационной выборке выросло до `0.5875` . Площадь под ROC кривой для данной модели составила `0.8346`, что говорит о достаточно хорошей разделяющей способности данной модели.  
    * Модель случайного леса показала лучший результат при максимальной глубине равной 15 и количестве решателей равным 95. Значение метрики F1 на валидационной выборке выросло до `0.6497` . Площадь под ROC кривой для данной модели составила `0.8541`, что говорит о хорошей разделяющей способности данной модели. Это лучший результат из всех трех моделей.  
    * Модель логистической регрессии, обученная на 100 итерациях, на валидационной выборке показала значение метрики F1 выросло до `0.4843` . Площадь под ROC кривой для данной модели составила `0.7604`. Разделяющая способность данной модели худшая из всех трех.  
    
Таким образом, лучший результат опять показала модель случайного леса и имеет лучшую разделяющую способнось.

3. С помощью уменьшения выборки. В выборке было осавлено 25% отрицательных классов. В результате удалось добиться того, что положительный класс составлял `50.6%` обучающей выборки. При данном методе модели показали следующие результаты: 
    * Модель решающего дерева показала лучший результат при максимальной глубине равной 5. Значение метрики F1 на валидационной выборке выросло до `0.5871` . Площадь под ROC кривой для данной модели составила `0.8114`, что говорит о достаточно хорошей разделяющей способности данной модели.  
    * Модель случайного леса показала лучший результат при максимальной глубине равной 8 и количестве решателей равным 50. Значение метрики F1 на валидационной выборке выросло до `0.6082` . Площадь под ROC кривой для данной модели составила `0.8545`, что говорит о хорошей разделяющей способности данной модели. Это лучший результат из всех трех моделей.  
    * Модель логистической регрессии, обученная на 100 итерациях, на валидационной выборке показала значение метрики F1 выросло до `0.4781` . Площадь под ROC кривой для данной модели составила `0.7597`. Разделяющая способность данной модели худшая из всех трех.  
    
Таким образом, лучший результат опять показала модель случайного леса и имеет лучшую разделяющую способнось.
  
В результате экспериментов был сделан вывод, что лучшей является модель случайного леса с максимальной глубиной 15 и количеством решателей равным 95, обучнная на сбалансированной выборке, полученной с помощью увеличения выборки. Именно данная модель была проверена на тестовой выборке.
  
На четвертом этапе лучшая модель была проверена на тестовой выборке. Значение метрики F1 оказалось равным 0.6028, что удовлетворяет поставленной задаче.    
  
Таким образом, рекомендуется использовать модель случайного леса с максимальной глубиной 15 и количеством решателей равным 95.